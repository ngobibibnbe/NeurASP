{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Addition (Single Digit)\n",
    "The digit addition problem is a simple but illustrative example used in (Manhaeve et al. 2018) to illustrate DeepProbLogâ€™s ability to do both logical reasoning and deep learning. We will also use this simple example to show how to use NeurASP.\n",
    "\n",
    "The task is, given a pair of digit images (MNIST) and their sum as the label, let a neural network learn the digit classification of the input images.\n",
    "## Data Format\n",
    "### dataList\n",
    "A list of Python dictionaries as shown below, each dictionary maps logic symbols (in the form of strings) to tensors. In this problem, each dictionary in dataList denotes one single training instance and maps i1 and i2 in the NeurASP program to 2 tensors. Each dataTensor is an image of size (1,28,28) in MNIST dataset. \n",
    "\n",
    "    [\n",
    "        {'i1' : dataTensor1, 'i2' : dataTensor2} ,\n",
    "        ... ,\n",
    "        {'i1' : dataTensor1', 'i2' : dataTensor2'}\n",
    "    ]\n",
    "\n",
    "### obsList\n",
    "A list of constraints (in the form of strings), whose probabilities are to be maximized during training. The length of obsList is the same as the length of dataList, which is also the number of training instances. An example element of obsList is shown below. \n",
    "\n",
    "    ':- not addition(i1, i2, 12).'\n",
    "\n",
    "### test_loader\n",
    "Pytorch DataLoader object consisting of 10 batches in the following format\n",
    "\n",
    "    [  \n",
    "        [dataTensors, labelTensors]  \n",
    "        ...  \n",
    "        [dataTensors, labelTensors]  \n",
    "    ]\n",
    "\n",
    "Here dataTensors is a list of 1000 tensor of size (1,28,28) for a digit image, and labelTensors is a list of 1000 corresponding labels in the form of integer tensors. \n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataGen import dataList, obsList, test_loader\n",
    "from network import Net\n",
    "#from neurasp import NeurASP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import clingo\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from mvpp import MVPP\n",
    "\n",
    "\n",
    "class NeurASP(object):\n",
    "    def __init__(self, dprogram, nnMapping, optimizers, gpu=True):\n",
    "\n",
    "        \"\"\"\n",
    "        @param dprogram: a string for a NeurASP program\n",
    "        @param nnMapping: a dictionary maps nn names to neural networks modules\n",
    "        @param optimizers: a dictionary maps nn names to their optimizers\n",
    "        @param gpu: a Boolean denoting whether the user wants to use GPU for training and testing\n",
    "        \"\"\"\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() and gpu else 'cpu')\n",
    "\n",
    "        self.dprogram = dprogram\n",
    "        self.const = {} # the mapping from c to v for rule #const c=v.\n",
    "        self.n = {} # the mapping from nn name to an integer n denoting the domain size; n would be 1 or N (>=3); note that n=2 in theorey is implemented as n=1\n",
    "        self.e = {} # the mapping from nn name to an integer e\n",
    "        self.domain = {} # the mapping from nn name to the domain of the predicate in that nn atom\n",
    "        self.normalProbs = None # record the probabilities from normal prob rules\n",
    "        self.nnOutputs = {}\n",
    "        self.nnGradients = {}\n",
    "        if gpu==True:\n",
    "            self.nnMapping = {key : nn.DataParallel(nnMapping[key].to(self.device)) for key in nnMapping}\n",
    "        else:\n",
    "            self.nnMapping = nnMapping\n",
    "        self.optimizers = optimizers\n",
    "        # self.mvpp is a dictionary consisting of 4 keys: \n",
    "        # 1. 'program': a string denoting an MVPP program where the probabilistic rules generated from NN are followed by other rules;\n",
    "        # 2. 'nnProb': a list of lists of tuples, each tuple is of the form (model, i ,term, j)\n",
    "        # 3. 'atom': a list of list of atoms, where each list of atoms is corresponding to a prob. rule\n",
    "        # 4. 'nnPrRuleNum': an integer denoting the number of probabilistic rules generated from NN\n",
    "        self.mvpp = {'nnProb': [], 'atom': [], 'nnPrRuleNum': 0, 'program': ''}\n",
    "        self.yoloInfo = [] # (beta version; used when e=\"yolo\") a list for yolo neural network to make prediction, each element is of the form [m, t, domain]\n",
    "        self.mvpp['program'], self.mvpp['program_pr'], self.mvpp['program_asp'] = self.parse(obs='')\n",
    "        self.stableModels = [] # a list of stable models, where each stable model is a list\n",
    "\n",
    "\n",
    "\n",
    "    def constReplacement(self, t):\n",
    "        \"\"\" Return a string obtained from t by replacing all c with v if '#const c=v.' is present\n",
    "\n",
    "        @param t: a string, which is a term representing an input to a neural network\n",
    "        \"\"\"\n",
    "        t = t.split(',')\n",
    "        t = [self.const[i.strip()] if i.strip() in self.const else i.strip() for i in t]\n",
    "        return ','.join(t)\n",
    "\n",
    "    def nnAtom2MVPPrules(self, nnAtom):\n",
    "        \"\"\"\n",
    "        @param nnAtom: a string of a neural atom\n",
    "        @param countIdx: a Boolean value denoting whether we count the index for the value of m(t, i)[j]\n",
    "        \"\"\"\n",
    "\n",
    "        # STEP 1: obtain all information\n",
    "        regex = '^nn\\((.+)\\((.+)\\),\\((.+)\\)\\)$'\n",
    "        out = re.search(regex, nnAtom)\n",
    "        m = out.group(1)\n",
    "        e, t = out.group(2).split(',', 1) # in case t is of the form t1,...,tk, we only split on the first comma\n",
    "        domain = out.group(3).split(',')\n",
    "        t = self.constReplacement(t)\n",
    "        # check the value of e\n",
    "        e = e.strip()\n",
    "        if e != 'yolo':\n",
    "            e = int(self.constReplacement(e))\n",
    "        n = len(domain)\n",
    "        if n == 2:\n",
    "            n = 1\n",
    "        self.n[m] = n\n",
    "        self.e[m] = e\n",
    "        self.domain[m] = domain\n",
    "        if m not in self.nnOutputs:\n",
    "            self.nnOutputs[m] = {}\n",
    "            self.nnGradients[m] = {}\n",
    "        if t not in self.nnOutputs[m]:\n",
    "            self.nnOutputs[m][t] = None\n",
    "            self.nnGradients[m][t] = None\n",
    "\n",
    "        # STEP 2: generate MVPP rules\n",
    "        mvppRules = []\n",
    "        # if the neural network is yolo network\n",
    "        if e == 'yolo':\n",
    "            self.yoloInfo.append((m, t, domain))\n",
    "        # if the neural network is a classification network that outputs probabilities only\n",
    "        else:\n",
    "            # we have different translations when n = 2 (i.e., n = 1 in implementation) or when n > 2\n",
    "            if n == 1:\n",
    "                for i in range(e):\n",
    "                    rule = '@0.0 {}({}, {}, {}); @0.0 {}({}, {}, {}).'.format(m, i, t, domain[0], m, i, t, domain[1])\n",
    "                    prob = [tuple((m, i, t, 0))]\n",
    "                    atoms = ['{}({}, {}, {})'.format(m, i, t, domain[0]), '{}({}, {}, {})'.format(m, i, t, domain[1])]\n",
    "                    mvppRules.append(rule)\n",
    "                    self.mvpp['nnProb'].append(prob)\n",
    "                    self.mvpp['atom'].append(atoms)\n",
    "                    self.mvpp['nnPrRuleNum'] += 1\n",
    "\n",
    "            elif n > 2:\n",
    "                for i in range(e):\n",
    "                    rule = ''\n",
    "                    prob = []\n",
    "                    atoms = []\n",
    "                    for j in range(n):\n",
    "                        atom = '{}({}, {}, {})'.format(m, i, t, domain[j])\n",
    "                        rule += '@0.0 {}({}, {}, {}); '.format(m, i, t, domain[j])\n",
    "                        prob.append(tuple((m, i, t, j)))\n",
    "                        atoms.append(atom)\n",
    "                    mvppRules.append(rule[:-2]+'.')\n",
    "                    self.mvpp['nnProb'].append(prob)\n",
    "                    self.mvpp['atom'].append(atoms)\n",
    "                    self.mvpp['nnPrRuleNum'] += 1\n",
    "            else:\n",
    "                print('Error: the number of element in the domain %s is less than 2' % domain)\n",
    "        return mvppRules\n",
    "\n",
    "\n",
    "    def parse(self, obs=''):\n",
    "        dprogram = self.dprogram + obs\n",
    "        # 1. Obtain all const definitions c for each rule #const c=v.\n",
    "        regex = '#const\\s+(.+)=(.+).'\n",
    "        out = re.search(regex, dprogram)\n",
    "        if out:\n",
    "            self.const[out.group(1).strip()] = out.group(2).strip()\n",
    "        # 2. Generate prob. rules for grounded nn atoms\n",
    "        clingo_control = clingo.Control([\"--warn=none\"])\n",
    "        # 2.1 remove weak constraints and comments\n",
    "        program = re.sub(r'\\n:~ .+\\.[ \\t]*\\[.+\\]', '\\n', dprogram)\n",
    "        program = re.sub(r'\\n%[^\\n]*', '\\n', program)\n",
    "        # 2.2 replace [] with ()\n",
    "        program = program.replace('[', '(').replace(']', ')')\n",
    "        # 2.3 use MVPP package to parse prob. rules and obtain ASP counter-part\n",
    "        mvpp = MVPP(program)\n",
    "        if mvpp.parameters and not self.normalProbs:\n",
    "            self.normalProbs = mvpp.parameters\n",
    "        pi_prime = mvpp.pi_prime\n",
    "        # 2.4 use clingo to generate all grounded NN atoms and turn them into prob. rules\n",
    "        clingo_control.add(\"base\", [], pi_prime)\n",
    "        clingo_control.ground([(\"base\", [])])\n",
    "        symbols = [atom.symbol for atom in clingo_control.symbolic_atoms]\n",
    "        mvppRules = [self.nnAtom2MVPPrules(str(atom)) for atom in symbols if atom.name == 'nn']\n",
    "        mvppRules = [rule for rules in mvppRules for rule in rules]\n",
    "        # 3. obtain the ASP part in the original NeurASP program\n",
    "        lines = [line.strip() for line in dprogram.split('\\n') if line and not line.startswith('nn(')]\n",
    "        return '\\n'.join(mvppRules + lines), '\\n'.join(mvppRules), '\\n'.join(lines)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def satisfy(model, asp):\n",
    "        \"\"\"\n",
    "        Return True if model satisfies the asp program; False otherwise\n",
    "        @param model: a stable model in the form of a list of atoms, where each atom is a string\n",
    "        @param asp: an ASP program (constraints) in the form of a string\n",
    "        \"\"\"\n",
    "        asp_with_facts = asp + '\\n'\n",
    "        for atom in model:\n",
    "            asp_with_facts += atom + '.\\n'\n",
    "        clingo_control = clingo.Control(['--warn=none'])\n",
    "        clingo_control.add('base', [], asp_with_facts)\n",
    "        clingo_control.ground([('base', [])])\n",
    "        result = clingo_control.solve()\n",
    "        if str(result) == 'SAT':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "        \n",
    "    def infer(self, dataDic, obs='', mvpp='', postProcessing=None):\n",
    "        \"\"\"\n",
    "        @param dataDic: a dictionary that maps terms to tensors/np-arrays\n",
    "        @param obs: a string which is a set of constraints denoting an observation\n",
    "        @param mvpp: an MVPP program used in inference\n",
    "        \"\"\"\n",
    "\n",
    "        mvppRules = ''\n",
    "        facts = ''\n",
    "\n",
    "        # Step 1: get the output of each neural network\n",
    "        for m in self.nnOutputs:\n",
    "            self.nnMapping[m].eval()\n",
    "            for t in self.nnOutputs[m]:\n",
    "                # if dataDic maps t to tuple (dataTensor, {'m': labelTensor})\n",
    "                if isinstance(dataDic[t], tuple):\n",
    "                    dataTensor = dataDic[t][0]\n",
    "                # if dataDic maps t to dataTensor directly\n",
    "                else:\n",
    "                    dataTensor = dataDic[t]\n",
    "                if self.e[m] == 'yolo':\n",
    "                    if postProcessing:\n",
    "                        self.nnOutputs[m][t] = postProcessing(self.nnMapping[m](dataTensor))\n",
    "                        # Step 2: turn yolo outputs into a set of MVPP probabilistic rules\n",
    "                        for idx, (label, x1, y1, x2, y2, conf) in enumerate(self.nnOutputs[m][t]):\n",
    "                            # we only include the box if its label is within domain\n",
    "                            if label in self.domain[m]:\n",
    "                                facts += 'box({}, b{}, {}, {}, {}, {}).\\n'.format(t, idx, x1, y1, x2, y2)\n",
    "                                atom = '{}({}, b{}, {})'.format(m, t, idx, label)\n",
    "                                mvppRules += '@{} {}; @{} -{}.\\n'.format(conf, atom, 1-conf, atom)\n",
    "                    else:\n",
    "                        print('Error: the function for postProcessing yolo output is not specified.')\n",
    "                        sys.exit()\n",
    "                else:\n",
    "                    self.nnOutputs[m][t] = self.nnMapping[m](dataTensor).view(-1).tolist()\n",
    "\n",
    "        # Step 3: turn the NN outputs (from usual classification neurual networks) into a set of MVPP probabilistic rules\n",
    "        for ruleIdx in range(self.mvpp['nnPrRuleNum']):\n",
    "            probs = [self.nnOutputs[m][t][i*self.n[m]+j] for (m, i, t, j) in self.mvpp['nnProb'][ruleIdx]]\n",
    "            if len(probs) == 1:\n",
    "                mvppRules += '@{} {}; @{} {}.\\n'.format(probs[0], self.mvpp['atom'][ruleIdx][0], 1 - probs[0], self.mvpp['atom'][ruleIdx][1])\n",
    "            else:\n",
    "                tmp = ''\n",
    "                for atomIdx, prob in enumerate(probs):\n",
    "                    tmp += '@{} {}; '.format(prob, self.mvpp['atom'][ruleIdx][atomIdx])\n",
    "                mvppRules += tmp[:-2] + '.\\n'\n",
    "\n",
    "        # Step 3: find an optimal SM under obs\n",
    "        dmvpp = MVPP(facts + mvppRules + mvpp)\n",
    "        return dmvpp.find_one_most_probable_SM_under_obs_noWC(obs=obs)  #. , obs\n",
    "\n",
    "\n",
    "    def learn(self, dataList, obsList, epoch, alpha=0, lossFunc='cross', method='exact', lr=0.01, opt=False, storeSM=False, smPickle=None, accEpoch=0, batchSize=1):\n",
    "        \"\"\"\n",
    "        @param dataList: a list of dictionaries, where each dictionary maps terms to either a tensor/np-array or a tuple (tensor/np-array, {'m': labelTensor})\n",
    "        @param obsList: a list of strings, where each string is a set of constraints denoting an observation\n",
    "        @param epoch: an integer denoting the number of epochs\n",
    "        @param alpha: a real number between 0 and 1 denoting the weight of cross entropy loss; (1-alpha) is the weight of semantic loss\n",
    "        @param lossFunc: a string in {'cross'} or a loss function object in pytorch\n",
    "        @param method: a string in {'exact', 'sampling'} denoting whether the gradients are computed exactly or by sampling\n",
    "        @param lr: a real number between 0 and 1 denoting the learning rate for the probabilities in probabilistic rules\n",
    "        @param batchSize: a positive interger denoting the batch size, i.e., how many data instances do we use to update the NN parameters for once\n",
    "        \"\"\"\n",
    "        assert len(dataList) == len(obsList), 'Error: the length of dataList does not equal to the length of obsList'\n",
    "        assert alpha >= 0 and alpha <= 1, 'Error: the value of alpha should be within [0, 1]'\n",
    "\n",
    "        # if the pickle file for stable models is given, we will either read all stable models from it or\n",
    "        # store all newly generated stable models in that pickle file in case the pickle file cannot be loaded\n",
    "        savePickle = False\n",
    "        if smPickle is not None:\n",
    "            storeSM = True\n",
    "            try:\n",
    "                with open(smPickle, 'rb') as fp:\n",
    "                    self.stableModels = pickle.load(fp)\n",
    "            except Exception:\n",
    "                savePickle = True\n",
    "\n",
    "\n",
    "        # get the mvpp program by self.mvpp, so far self.mvpp['program'] is a string\n",
    "        if method == 'nn_prediction':\n",
    "            dmvpp = MVPP(self.mvpp['program_pr'])\n",
    "        elif method == 'penalty':\n",
    "            dmvpp = MVPP(self.mvpp['program_pr'])\n",
    "        else:\n",
    "            dmvpp = MVPP(self.mvpp['program'])\n",
    "\n",
    "        # we train all nerual network models\n",
    "        for m in self.nnMapping:\n",
    "            self.nnMapping[m].train()\n",
    "\n",
    "        # we train for 'epoch' times of epochs\n",
    "        for epochIdx in range(epoch):\n",
    "            # for each training instance in the training data\n",
    "            for dataIdx, data in enumerate(dataList):\n",
    "                #print(\"----\",dataIdx)\n",
    "                # data is a dictionary. we need to edit its key if the key contains a defined const c\n",
    "                # where c is defined in rule #const c=v.\n",
    "                for key in data:\n",
    "                    data[self.constReplacement(key)] = data.pop(key)\n",
    "\n",
    "                # Step 1: get the output of each neural network and initialize the gradients\n",
    "                for m in self.nnOutputs:\n",
    "                    nnOutput = {}\n",
    "                    nnOutput[m] = {}\n",
    "                    for t in self.nnOutputs[m]:\n",
    "                        labelTensor = None\n",
    "                        # if data maps t to tuple (dataTensor, {'m': labelTensor})\n",
    "                        if isinstance(data[t], tuple):\n",
    "                            dataTensor = data[t][0]\n",
    "                            if m in data[t][1]:\n",
    "                                labelTensor = data[t][1][m]\n",
    "                        # if data maps t to dataTensor directly\n",
    "                        else:\n",
    "                            dataTensor = data[t]\n",
    "\n",
    "                        nnOutput[m][t] = self.nnMapping[m](dataTensor.to(self.device))\n",
    "                        nnOutput[m][t] = torch.clamp(nnOutput[m][t], min=10e-8, max=1.-10e-8)\n",
    "\n",
    "                        self.nnOutputs[m][t] = nnOutput[m][t].view(-1).tolist()\n",
    "                        # initialize the semantic gradients for each output\n",
    "                        self.nnGradients[m][t] = [0.0 for i in self.nnOutputs[m][t]]\n",
    "\n",
    "                        # if alpha is greater than 0 and the labelTensor is given in dataList, we compute the nn gradients\n",
    "                        if alpha > 0 and labelTensor is not None:\n",
    "                            if isinstance(lossFunc, str):\n",
    "                                if lossFunc == 'cross':\n",
    "                                    criterion = torch.nn.NLLLoss()\n",
    "                                    loss = alpha * criterion(torch.log(nnOutput[m][t].view(-1, self.n[m])), labelTensor.long().view(-1))\n",
    "                            else:\n",
    "                                criterion = lossFunc\n",
    "                                loss = alpha * criterion(nnOutput[m][t].view(-1, self.n[m]), labelTensor)\n",
    "                            loss.backward(retain_graph=True)\n",
    "\n",
    "                # Step 2: if alpha is less than 1, we compute the semantic gradients\n",
    "                if alpha < 1:\n",
    "                    # Step 2.1: replace the parameters in the MVPP program with nn outputs\n",
    "                    for ruleIdx in range(self.mvpp['nnPrRuleNum']):\n",
    "                        dmvpp.parameters[ruleIdx] = [self.nnOutputs[m][t][i*self.n[m]+j] for (m, i, t, j) in self.mvpp['nnProb'][ruleIdx]]\n",
    "                        if len(dmvpp.parameters[ruleIdx]) == 1:\n",
    "                            dmvpp.parameters[ruleIdx] = [dmvpp.parameters[ruleIdx][0], 1-dmvpp.parameters[ruleIdx][0]]\n",
    "\n",
    "                    # Step 2.2: replace the parameters for normal prob. rules in the MVPP program with updated probabilities\n",
    "                    if self.normalProbs:\n",
    "                        for ruleIdx, probs in enumerate(self.normalProbs):\n",
    "                            dmvpp.parameters[self.mvpp['nnPrRuleNum']+ruleIdx] = probs\n",
    "\n",
    "                    # Step 2.3: compute the gradients\n",
    "                    dmvpp.normalize_probs()\n",
    "                    check = False\n",
    "                    if storeSM:\n",
    "                        try:\n",
    "                            models = self.stableModels[dataIdx]\n",
    "                            gradients = dmvpp.mvppLearn(models)\n",
    "                        except:\n",
    "                            if opt:\n",
    "                                models = dmvpp.find_all_opt_SM_under_obs_WC(obsList[dataIdx])\n",
    "                            else:\n",
    "                                models = dmvpp.find_k_SM_under_obs(obsList[dataIdx], k=0)\n",
    "                            self.stableModels.append(models)\n",
    "                            gradients = dmvpp.mvppLearn(models)\n",
    "                    else:\n",
    "                        if method == 'exact':\n",
    "                            gradients = dmvpp.gradients_one_obs(obsList[dataIdx], opt=opt)\n",
    "                        elif method == 'sampling':\n",
    "                            models = dmvpp.sample_obs(obsList[dataIdx], num=10)\n",
    "                            gradients = dmvpp.mvppLearn(models)\n",
    "                        elif method == 'nn_prediction': \n",
    "                            models = dmvpp.find_one_most_probable_SM_under_obs_noWC()\n",
    "                            check = self.satisfy(models[0], self.mvpp['program_asp'] + obsList[dataIdx])\n",
    "                            gradients = dmvpp.mvppLearn(models) if check else -dmvpp.mvppLearn(models)\n",
    "                            if check:\n",
    "                                continue\n",
    "                        elif method == 'penalty':\n",
    "                            models = dmvpp.find_all_SM_under_obs()\n",
    "                            models_noSM = [model for model in models if not self.satisfy(model, self.mvpp['program_asp'] + obsList[dataIdx])]\n",
    "                            gradients = - dmvpp.mvppLearn(models_noSM)\n",
    "                        else:\n",
    "                            print('Error: the method \\'%s\\' should be either \\'exact\\' or \\'sampling\\'', method)\n",
    "\n",
    "                    # Step 2.4: update parameters in neural networks\n",
    "                    gradientsNN = gradients[:self.mvpp['nnPrRuleNum']].tolist()\n",
    "                    for ruleIdx in range(self.mvpp['nnPrRuleNum']):\n",
    "                        for probIdx, (m, i, t, j) in enumerate(self.mvpp['nnProb'][ruleIdx]):\n",
    "                            self.nnGradients[m][t][i*self.n[m]+j] = (alpha - 1) * gradientsNN[ruleIdx][probIdx]\n",
    "                    # Step 2.5: backpropogate\n",
    "                    for m in nnOutput:\n",
    "                        for t in nnOutput[m]:\n",
    "                            if self.device.type == 'cuda':\n",
    "                                nnOutput[m][t].backward(torch.cuda.FloatTensor(np.reshape(np.array(self.nnGradients[m][t]),nnOutput[m][t].shape)), retain_graph=True)\n",
    "                            else:\n",
    "                                nnOutput[m][t].backward(torch.FloatTensor(np.reshape(np.array(self.nnGradients[m][t]),nnOutput[m][t].shape)), retain_graph=True)\n",
    "\n",
    "                # Step 3: update the parameters\n",
    "                if (dataIdx+1) % batchSize == 0:\n",
    "                    for m in self.optimizers:\n",
    "                        self.optimizers[m].step()\n",
    "                        self.optimizers[m].zero_grad()\n",
    "\n",
    "                # Step 4: if alpha is less than 1, we update probabilities in normal prob. rules\n",
    "                if alpha < 1:\n",
    "                    if self.normalProbs:\n",
    "                        gradientsNormal = gradients[self.mvpp['nnPrRuleNum']:].tolist()\n",
    "                        for ruleIdx, ruleGradients in enumerate(gradientsNormal):\n",
    "                            ruleIdxMVPP = self.mvpp['nnPrRuleNum']+ruleIdx\n",
    "                            for atomIdx, b in enumerate(dmvpp.learnable[ruleIdxMVPP]):\n",
    "                                if b == True:\n",
    "                                    dmvpp.parameters[ruleIdxMVPP][atomIdx] += lr * gradientsNormal[ruleIdx][atomIdx]\n",
    "                        dmvpp.normalize_probs()\n",
    "                        self.normalProbs = dmvpp.parameters[self.mvpp['nnPrRuleNum']:]\n",
    "\n",
    "                # Step 5: show training accuracy\n",
    "                if accEpoch !=0 and (dataIdx+1) % accEpoch == 0:\n",
    "                    print('Training accuracy at interation {}:'.format(dataIdx+1))\n",
    "                    self.testConstraint(dataList, obsList, [self.mvpp['program']])\n",
    "                printed_acc, _ = self.testNN('digit', test_loader)\n",
    "                #print(\"---------------------mon print accuracy=\", printed_acc)\n",
    "                \n",
    "                if dataIdx%1000==0:\n",
    "                    writer.add_scalar(\"NeurASP\",printed_acc,dataIdx)\n",
    "                    print(dataIdx)\n",
    "\n",
    "            # Step 6: save the stable models in a pickle file for potentially later usage\n",
    "            if savePickle:\n",
    "                with open(smPickle, 'wb') as fp:\n",
    "                    pickle.dump(self.stableModels, fp)\n",
    "                savePickle = False\n",
    "\n",
    "    def testNN(self, nn, testLoader):\n",
    "        \"\"\"\n",
    "        Return a real number in [0,100] denoting accuracy\n",
    "        @nn is the name of the neural network to check the accuracy. \n",
    "        @testLoader is the input and output pairs.\n",
    "        \"\"\"\n",
    "        self.nnMapping[nn].eval()\n",
    "        # check if total prediction is correct\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        # check if each single prediction is correct\n",
    "        singleCorrect = 0\n",
    "        singleTotal = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in testLoader:\n",
    "                output = self.nnMapping[nn](data.to(self.device))\n",
    "                if self.n[nn] > 2 :\n",
    "                    pred = output.argmax(dim=-1, keepdim=True) # get the index of the max log-probability\n",
    "                    target = target.to(self.device).view_as(pred)\n",
    "                    correctionMatrix = (target.int() == pred.int()).view(target.shape[0], -1)\n",
    "                    correct += correctionMatrix.all(1).sum().item()\n",
    "                    total += target.shape[0]\n",
    "                    singleCorrect += correctionMatrix.sum().item()\n",
    "                    singleTotal += target.numel()\n",
    "                else: \n",
    "                    pred = np.array([int(i[0]<0.5) for i in output.tolist()])\n",
    "                    target = target.numpy()\n",
    "                    correct += (pred.reshape(target.shape) == target).sum()\n",
    "                    total += len(pred)\n",
    "        accuracy = 100. * correct / total\n",
    "        singleAccuracy = 100. * singleCorrect / singleTotal\n",
    "        return accuracy, singleAccuracy\n",
    "    \n",
    "    # We interprete the most probable stable model(s) as the prediction of the inference mode\n",
    "    # and check the accuracy of the inference mode by checking whether the obs is satisfied by the prediction\n",
    "    def testInferenceResults(self, dataList, obsList):\n",
    "        \"\"\" Return a real number in [0,1] denoting the accuracy\n",
    "        @param dataList: a list of dictionaries, where each dictionary maps terms to tensors/np-arrays\n",
    "        @param obsList: a list of strings, where each string is a set of constraints denoting an observation\n",
    "        \"\"\"\n",
    "        assert len(dataList) == len(obsList), 'Error: the length of dataList does not equal to the length of obsList'\n",
    "\n",
    "        correct = 0\n",
    "        for dataIdx, data in enumerate(dataList):\n",
    "            models = self.infer(data, obs=':- mistake.', mvpp=self.mvpp['program_asp'])\n",
    "            for model in models:\n",
    "                if self.satisfy(model, obsList[dataIdx]):\n",
    "                    correct += 1\n",
    "                    break\n",
    "        accuracy = 100. * correct / len(dataList)\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "    def testConstraint(self, dataList, obsList, mvppList):\n",
    "        \"\"\"\n",
    "        @param dataList: a list of dictionaries, where each dictionary maps terms to tensors/np-arrays\n",
    "        @param obsList: a list of strings, where each string is a set of constraints denoting an observation\n",
    "        @param mvppList: a list of MVPP programs (each is a string)\n",
    "        \"\"\"\n",
    "        assert len(dataList) == len(obsList), 'Error: the length of dataList does not equal to the length of obsList'\n",
    "\n",
    "        # we evaluate all nerual networks\n",
    "        for func in self.nnMapping:\n",
    "            self.nnMapping[func].eval()\n",
    "\n",
    "        # we count the correct prediction for each mvpp program\n",
    "        count = [0]*len(mvppList)\n",
    "\n",
    "        for dataIdx, data in enumerate(dataList):\n",
    "            # data is a dictionary. we need to edit its key if the key contains a defined const c\n",
    "            # where c is defined in rule #const c=v.\n",
    "            for key in data:\n",
    "                data[self.constReplacement(key)] = data.pop(key)\n",
    "\n",
    "            # Step 1: get the output of each neural network\n",
    "            for model in self.nnOutputs:\n",
    "                for t in self.nnOutputs[model]:\n",
    "                    self.nnOutputs[model][t] = self.nnMapping[model](data[t].to(self.device)).view(-1).tolist()\n",
    "\n",
    "            # Step 2: turn the NN outputs into a set of ASP facts\n",
    "            aspFacts = ''\n",
    "            for ruleIdx in range(self.mvpp['nnPrRuleNum']):\n",
    "                probs = [self.nnOutputs[m][t][i*self.n[model]+j] for (m, i, t, j) in self.mvpp['nnProb'][ruleIdx]]\n",
    "                if len(probs) == 1:\n",
    "                    atomIdx = int(probs[0] < 0.5) # t is of index 0 and f is of index 1\n",
    "                else:\n",
    "                    atomIdx = probs.index(max(probs))\n",
    "                aspFacts += self.mvpp['atom'][ruleIdx][atomIdx] + '.\\n'\n",
    "\n",
    "            # Step 3: check whether each MVPP program is satisfied\n",
    "            for programIdx, program in enumerate(mvppList):\n",
    "                # if the program has weak constraints\n",
    "                if re.search(r':~.+\\.[ \\t]*\\[.+\\]', program) or re.search(r':~.+\\.[ \\t]*\\[.+\\]', obsList[dataIdx]):\n",
    "                    choiceRules = ''\n",
    "                    for ruleIdx in range(self.mvpp['nnPrRuleNum']):\n",
    "                        choiceRules += '1{' + '; '.join(self.mvpp['atom'][ruleIdx]) + '}1.\\n'\n",
    "                    mvpp = MVPP(program+choiceRules)\n",
    "                    models = mvpp.find_all_opt_SM_under_obs_WC(obs=obsList[dataIdx])\n",
    "                    models = [set(model) for model in models] # each model is a set of atoms\n",
    "                    targetAtoms = aspFacts.split('.\\n')\n",
    "                    targetAtoms = set([atom.strip().replace(' ','') for atom in targetAtoms if atom.strip()])\n",
    "                    if any(targetAtoms.issubset(model) for model in models):\n",
    "                        count[programIdx] += 1\n",
    "                else:\n",
    "                    mvpp = MVPP(aspFacts + program)\n",
    "                    if mvpp.find_one_SM_under_obs(obs=obsList[dataIdx]):\n",
    "                        count[programIdx] += 1\n",
    "        for programIdx, program in enumerate(mvppList):\n",
    "            print('The accuracy for constraint {} is {}'.format(programIdx+1, float(count[programIdx])/len(dataList)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeurASP Program\n",
    "The NeurASP program can be written in the form of ''' Rules '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprogram = '''\n",
    "img(i1). img(i2).\n",
    "addition(A,B,N) :- digit(0,A,N1), digit(0,B,N2), N=N1+N2.\n",
    "nn(digit(1,X), [0,1,2,3,4,5,6,7,8,9]) :- img(X).\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Instantiation\n",
    "- Instantiate neural networks.\n",
    "- Define nnMapping: a dictionary that maps neural network names (i.e., strings) to the neural network objects (i.e., torch.nn.Module object)\n",
    "- Define optimizers: a dictionary that maps neural network names (i.e., strings) to optimizer (we use the Adam optimizer here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Net()\n",
    "nnMapping = {'digit': m}\n",
    "optimizers = {'digit': torch.optim.Adam(m.parameters(), lr=0.001)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NeurASP Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeurASPobj = NeurASP(dprogram, nnMapping, optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "saveModelPath = 'data/model.pt'\n",
    "startTime = time.time()\n",
    "time1 = time.time()\n",
    "\n",
    "for i in range(1):\n",
    "    print('Epoch {}...'.format(i+1))\n",
    "    NeurASPobj.learn(dataList=dataList, obsList=obsList, epoch=1, smPickle='data/stableModels.pickle')\n",
    "    acc, _ = NeurASPobj.testNN('digit', test_loader)\n",
    "    #writer.add_scalar(\"NeurASP\",acc,i)\n",
    "    \n",
    "\n",
    "print(len(dataList))\n",
    "time2 = time.time()\n",
    "print('--- train time: %s seconds ---' % (time2 - time1))\n",
    "print('--- test time: %s seconds ---' % (time.time() - time2))\n",
    "print('--- total time from beginning: %s minutes ---' % int((time.time() - startTime)/60) )\n",
    "print('Storing the trained model into {}'.format(saveModelPath))\n",
    "torch.save(m.state_dict(), saveModelPath)\n",
    "print('Test Acc: {:0.2f}%'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeurASP vs DeepProbLog vs CNN\n",
    "The following image shows the accuracy on the test data after each iteration of training.\n",
    "The method CNN denotes the baseline introduced in (Manhaeve et al. 2018) where a convolutional neural network (with more parameters) is trained to classify the concatenation of the two images into the 19 possible sums. \n",
    "\n",
    "As we can see, the neural networks trained by NeurASP and DeepProbLog converge much\n",
    "faster than CNN and have almost the same accuracy at each iteration. However, NeurASP\n",
    "spends much less time for training compared to DeepProbLog. The time reported is for one\n",
    "epoch (30,000 iterations in gradient descent). This is because DeepProbLog constructs an\n",
    "SDD (Sequential Decision Diagram) at each iteration for each training instance (i.e., each pair of images) while SDD construction is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "path = 'data/NeurASPvsDeepProbLog.png'\n",
    "display(Image.open(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
