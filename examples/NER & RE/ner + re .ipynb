{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####*****************************************etape 1 Observation des resultats de SPERT et des vrais resultats par phrase\n",
    "import json\n",
    "import pandas as pd\n",
    "entities_types =[\"No\",\"Loc\",\"Org\", \"Peop\",\"Other\"]\n",
    "relations_types =[\"Work_For\", \"Kill\", \"OrgBased_In\", \"Live_In\", \"Located_In\"]\n",
    "\n",
    "\n",
    "\n",
    "print(type(json))\n",
    "with open(\"CONLL04\\conll04_test.json\") as file :\n",
    "    data_reals = json.load(file)\n",
    "with open(\"CONLL04/logs/spert_treshold_0.4/neurASP-SUP_predictions_valid_epoch_20.json\") as file :\n",
    "    #Scierc/logs/scierc_treshold_0.1/predictions_valid_epoch_1.json\n",
    "    data_preds = json.load(file)\n",
    "    #backup/scierc_treshold_0.1/predictions_valid_epoch_1.json\n",
    "def study (data_reals,data_preds):\n",
    "    longn1 =0\n",
    "    longn2 =0\n",
    "    longn3 =0\n",
    "    longr3 =0\n",
    "    longr2 =0\n",
    "    longr1 =0\n",
    "    longnr3 =0\n",
    "    longnr2 =0\n",
    "    longnr1 =0\n",
    "    longr =0\n",
    "    longn=0\n",
    "    longnr=0\n",
    "    for doc_id , doc_real in enumerate(data_reals):\n",
    "        entities_pred ={}\n",
    "        entities_real ={}    \n",
    "        for entity in doc_real[\"entities\"]:\n",
    "            entity_pos= (entity[\"start\"],entity[\"end\"])\n",
    "            entity_words = (' '.join(doc_real[\"tokens\"][entity[\"start\"]:entity[\"end\"]]))\n",
    "            entities_real[str(entity_pos)] = (entity_words, entity[\"type\"].lower())\n",
    "        for entity in data_preds[doc_id][\"entities\"]:\n",
    "            entity_pos= (entity[\"start\"],entity[\"end\"])\n",
    "            entity_words = (' '.join(doc_real[\"tokens\"][entity[\"start\"]:entity[\"end\"]]))\n",
    "            entity_probs = entity[\"probs\"].split(\"[\")[1].split(\"]\")[0]\n",
    "            probs =np.fromstring(entity_probs, dtype=float, sep=' ')\n",
    "            prob =probs[entities_types.index(entity['type'])]\n",
    "            entities_pred[str(entity_pos)] =(prob , (entity_words, entity[\"type\"].lower()) )\n",
    "\n",
    "        for idx in entities_real:\n",
    "            if idx not in entities_pred:\n",
    "                entities_pred[idx]=(0,\"0***\")\n",
    "        for idx in entities_pred:\n",
    "            if idx not in entities_real:\n",
    "                entities_real[idx]=(0,\"0***\")\n",
    "        display_real=[]\n",
    "        display_pred =[]\n",
    "        positions=[]\n",
    "        for idx in entities_real :\n",
    "            display_real.append(entities_real[idx])\n",
    "            display_pred.append(entities_pred[idx])\n",
    "            positions.append(idx)\n",
    "            longn+=1\n",
    "            if entities_real[idx]!=entities_pred[idx][1] and entities_real[idx][1]!=\"0***\"  and entities_pred[idx][1]!=\"0***\" :\n",
    "                longn3+=1\n",
    "            if entities_real[idx]!=entities_pred[idx][1] and  entities_pred[idx][1]==\"0***\" :\n",
    "                longn2+=1  # il n'identifie pas l'entité\n",
    "            if entities_real[idx]!=entities_pred[idx][1] and  entities_real[idx][1]==\"0***\" :\n",
    "                longn1+=1  # il identifie l'entité \n",
    "        df ={\"position\":positions , \"real\":display_real , \"predictions\":display_pred}\n",
    "        df =pd.DataFrame(data=df)\n",
    "        print(\"doc n°:\",doc_id)\n",
    "        print(df)\n",
    "\n",
    "        \n",
    "        relations_pred ={}\n",
    "        relations_real ={}    \n",
    "        for relation in doc_real[\"relations\"]:\n",
    "            relation_pos= (doc_real[\"entities\"][relation[\"head\"]][\"start\"], doc_real[\"entities\"][relation[\"head\"]][\"end\"]\n",
    "                           ,doc_real[\"entities\"][relation[\"tail\"]][\"start\"],doc_real[\"entities\"][relation[\"tail\"]][\"end\"])\n",
    "            relation_words = (' '.join(doc_real[\"tokens\"][doc_real[\"entities\"][relation[\"head\"]][\"start\"]:doc_real[\"entities\"][relation[\"head\"]][\"end\"]]),\n",
    "                           ' '.join(doc_real[\"tokens\"][doc_real[\"entities\"][relation[\"tail\"]][\"start\"]:doc_real[\"entities\"][relation[\"tail\"]][\"end\"]]))\n",
    "            relations_real[str(relation_pos)] = (relation_words, relation[\"type\"].lower())\n",
    "        for relation in data_preds[doc_id][\"relations\"]:\n",
    "            relation_pos= (data_preds[doc_id][\"entities\"][relation[\"head\"]][\"start\"], data_preds[doc_id][\"entities\"][relation[\"head\"]][\"end\"]\n",
    "                           ,data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"start\"],data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"end\"])\n",
    "            relation_words = (' '.join(doc_real[\"tokens\"][data_preds[doc_id][\"entities\"][relation[\"head\"]][\"start\"]:data_preds[doc_id][\"entities\"][relation[\"head\"]][\"end\"]]),\n",
    "                   ' '.join(doc_real[\"tokens\"][data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"start\"]:data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"end\"]]))\n",
    "            relation_probs = relation[\"probs\"].split(\"[\")[1].split(\"]\")[0]\n",
    "            probs =np.fromstring(relation_probs, dtype=float, sep=' ')\n",
    "            prob =probs[relations_types.index(relation['type'])]\n",
    "            relations_pred[str(relation_pos)] = (prob, relation_words, relation[\"type\"].lower())\n",
    "\n",
    "        for idx in relations_real:\n",
    "            if idx not in relations_pred:\n",
    "                relations_pred[idx]=(0,\"0***\")\n",
    "        for idx in relations_pred:\n",
    "            if idx not in relations_real:\n",
    "                relations_real[idx]=(0,\"0***\")\n",
    "        display_real=[]\n",
    "        display_pred =[]\n",
    "        positions=[]\n",
    "        for idx in relations_real :\n",
    "            display_real.append(relations_real[idx])\n",
    "            display_pred.append(relations_pred[idx])\n",
    "            positions.append(idx)\n",
    "            longr+=1\n",
    "            if relations_real[idx]!=relations_pred[idx][1:] and relations_real[idx][1]!=\"0***\"  and relations_pred[idx][1]!=\"0***\" :\n",
    "                longr3+=1\n",
    "            if relations_real[idx]!=relations_pred[idx][1:] and  relations_pred[idx][1]==\"0***\" :\n",
    "                longr2+=1  # il n'identifie pas l'entité\n",
    "            if relations_real[idx]!=relations_pred[idx][1:] and  relations_real[idx][1]==\"0***\" :\n",
    "                longr1+=1  # il identifie l'entité \n",
    "        \n",
    "        df ={\"position\":positions , \"real\":display_real , \"predictions\":display_pred}\n",
    "        df =pd.DataFrame(data=df)\n",
    "        print(\"doc n°:\",doc_id)\n",
    "        print(df)\n",
    "        \n",
    "        \n",
    "        \n",
    "        relations_pred ={}\n",
    "        relations_real ={}    \n",
    "        for relation in doc_real[\"relations\"]:\n",
    "            relation_pos= (doc_real[\"entities\"][relation[\"head\"]][\"start\"], doc_real[\"entities\"][relation[\"head\"]][\"end\"]\n",
    "                           ,doc_real[\"entities\"][relation[\"tail\"]][\"start\"],doc_real[\"entities\"][relation[\"tail\"]][\"end\"])\n",
    "            relation_words = (' '.join(doc_real[\"tokens\"][doc_real[\"entities\"][relation[\"head\"]][\"start\"]:doc_real[\"entities\"][relation[\"head\"]][\"end\"]]),\n",
    "                           ' '.join(doc_real[\"tokens\"][doc_real[\"entities\"][relation[\"tail\"]][\"start\"]:doc_real[\"entities\"][relation[\"tail\"]][\"end\"]]))\n",
    "            type_entity1= doc_real[\"entities\"][relation[\"head\"]][\"type\"]\n",
    "            type_entity2=doc_real[\"entities\"][relation[\"tail\"]][\"type\"]\n",
    "            relations_real[str(relation_pos)] = (relation_words, relation[\"type\"].lower(), type_entity1.lower(),type_entity2.lower())\n",
    "        for relation in data_preds[doc_id][\"relations\"]:\n",
    "            relation_pos= (data_preds[doc_id][\"entities\"][relation[\"head\"]][\"start\"], data_preds[doc_id][\"entities\"][relation[\"head\"]][\"end\"]\n",
    "                           ,data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"start\"],data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"end\"])\n",
    "            relation_words = (' '.join(doc_real[\"tokens\"][data_preds[doc_id][\"entities\"][relation[\"head\"]][\"start\"]:data_preds[doc_id][\"entities\"][relation[\"head\"]][\"end\"]]),\n",
    "                   ' '.join(doc_real[\"tokens\"][data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"start\"]:data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"end\"]]))\n",
    "            relation_probs = relation[\"probs\"].split(\"[\")[1].split(\"]\")[0]\n",
    "            probs =np.fromstring(relation_probs, dtype=float, sep=' ')\n",
    "            prob =probs[relations_types.index(relation['type'])]\n",
    "            \n",
    "            type_entity1= data_preds[doc_id][\"entities\"][relation[\"head\"]][\"type\"]\n",
    "            type_entity2=data_preds[doc_id][\"entities\"][relation[\"tail\"]][\"type\"]\n",
    "            relations_pred[str(relation_pos)] = (prob, relation_words, relation[\"type\"].lower(), type_entity1.lower(),type_entity2.lower())\n",
    "\n",
    "        for idx in relations_real:\n",
    "            if idx not in relations_pred:\n",
    "                relations_pred[idx]=(0,\"0***\")\n",
    "        for idx in relations_pred:\n",
    "            if idx not in relations_real:\n",
    "                relations_real[idx]=(0,\"0***\")\n",
    "        display_real=[]\n",
    "        display_pred =[]\n",
    "        positions=[]\n",
    "        for idx in relations_real :\n",
    "            display_real.append(relations_real[idx])\n",
    "            display_pred.append(relations_pred[idx])\n",
    "            positions.append(idx)\n",
    "            longnr+=1\n",
    "            if relations_real[idx]!=relations_pred[idx][1:] and relations_real[idx][1]!=\"0***\"  and relations_pred[idx][1]!=\"0***\" :\n",
    "                longnr3+=1\n",
    "            if relations_real[idx]!=relations_pred[idx][1:] and  relations_pred[idx][1]==\"0***\" :\n",
    "                longnr2+=1  # il n'identifie pas l'entité\n",
    "            if relations_real[idx]!=relations_pred[idx][1:] and  relations_real[idx][1]==\"0***\" :\n",
    "                longnr1+=1  # il identifie l'entité \n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            \"\"\"if relations_pred[idx] !=  relations_real[idx] and  relations_real[idx]==\"0***\"  :\n",
    "                display_real.append(relations_real[idx])\n",
    "                display_pred.append(relations_pred[idx])\n",
    "                positions.append(idx)\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        df ={\"position\":positions , \"real\":display_real , \"predictions\":display_pred}\n",
    "        df =pd.DataFrame(data=df)\n",
    "        print(\"doc n°:\",doc_id)\n",
    "        print(df)\n",
    "        print(\"(\", longn3,\"**\",longn2,\"**\",longn1,\"**\",longn,\"**) (\",longr3,\"**\",longr2,\"**\",longr1,\"**\",longr,\"**) (\",longnr3,\"**\",longnr2,\"**\",longnr1,\"**\",longnr,\")\" \"****************************************************separation***********************\")\n",
    "        \n",
    "study(data_reals,data_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CONLL04\\conll04_test.json\") as file :\n",
    "    data_reals = json.load(file)\n",
    "files =glob.glob(\"CONLL04/logs/spert_treshold_0.4/neurASP-SUP_predictions*.json\")\n",
    "for file_ in files :\n",
    "    print(file_)\n",
    "    with open(file_) as file :\n",
    "        data_preds = json.load(file)\n",
    "    print(file_)\n",
    "    study(data_reals,data_preds)\n",
    "\"\"\"\n",
    "\n",
    "print(type(json))\n",
    "with open(\"CONLL04/logs/spert_treshold_0.4/neurASP-SUP_predictions_valid_epoch_20.json\") as file :\n",
    "    data_reals = json.load(file)\n",
    "with open(\"CONLL04/logs/spert_treshold_0.4/predictions_valid_epoch_20.json\") as file :\n",
    "    #Scierc/logs/scierc_treshold_0.1/predictions_valid_epoch_1.json\n",
    "    data_preds = json.load(file)\n",
    "    \n",
    "study(data_reals,data_preds)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cleaning  ******************** \n",
    "#****************************************************************************************************************************\n",
    "# les tests se font maintenant de façon automatique avec automated_evaluation.py\n",
    "\n",
    "# we clean a files to have great NER \n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "from ast import literal_eval\n",
    "def decapitalize(str):\n",
    "    return str[:1].lower() + str[1:]\n",
    "  \n",
    "\n",
    "thresholds=[0.1,0.3]\n",
    "with open(\"Scierc/scierc_test.json\") as file :\n",
    "    data_real = json.load(file)\n",
    "types =[ \"task\", \"method\", \"material\", \"otherScientificTerm\",\"metric\",\"generic\"]           \n",
    "#print(\"logs\\spert_treshold_\"+str(threshold)+\"/*.json\") \n",
    "for threshold in thresholds:\n",
    "    files =glob.glob(\"Scierc\\logs\\scierc_treshold_\"+str(threshold)+\"\\predictions*.json\")\n",
    "    for file_ in files :\n",
    "        #file_ =\"Scierc/logs/scierc_treshold_0.1/predictions_valid_epoch_1.json\"\n",
    "        with open(file_) as file :\n",
    "            data_JSON = json.load(file)   \n",
    "            print(\"ok\")\n",
    "        for doc_id, document in enumerate(data_JSON) :\n",
    "            entities=document[\"entities\"]\n",
    "            for idx, entity in enumerate(entities):\n",
    "                flag =True \n",
    "                for real_id, real_entity in enumerate(data_real[doc_id][\"entities\"])  :\n",
    "                    if (entity[\"start\"]== real_entity[\"start\"]) and (entity[\"end\"]== real_entity[\"end\"]):\n",
    "                        entity_probs = entity[\"probs\"].split(\"[\")[1].split(\"]\")[0]\n",
    "                        probs =np.fromstring(entity_probs, dtype=float, sep=' ')\n",
    "                        t=len(probs)\n",
    "                        #print(probs)\n",
    "                        data_JSON[doc_id][\"entities\"][idx][\"type\"]=real_entity[\"type\"]\n",
    "                        probs =np.random.rand(t) * 0.1 \n",
    "                        probs[1+ types.index(decapitalize(real_entity[\"type\"]))] = 0.9\n",
    "                        #print( types.index(decapitalize(real_entity[\"type\"])))\n",
    "                        data_JSON[doc_id][\"entities\"][idx][\"probs\"]=str(probs.tolist()).replace(',','')\n",
    "                        flag =False \n",
    "                if flag:\n",
    "                    data_JSON[doc_id][\"entities\"][idx][\"type\"]=\"no\"\n",
    "                    entity_probs = entity[\"probs\"].split(\"[\")[1].split(\"]\")[0]\n",
    "                    probs =np.fromstring(entity_probs, dtype=float, sep=' ')\n",
    "                    probs =np.random.rand(len(probs)) * 0.01 \n",
    "                    probs[0] = 0.9\n",
    "                    data_JSON[doc_id][\"entities\"][idx][\"probs\"]=str(probs.tolist()).replace(',','')\n",
    "        #print(data_JSON)\n",
    "\n",
    "        with open(file_, 'w') as f:\n",
    "            json.dump(data_JSON, f)\n",
    "                #print(\"*********** You can actually test your model with evaluation functions\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### enlever la classe no aussi sur les predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "\n",
    "with open(\"Scierc/scierc_train_dev.json\") as file :\n",
    "    data_JSON1 = json.load(file)\n",
    "    \n",
    "with open(\"Scierc/scierc_test.json\") as file :\n",
    "    data_JSON2 = json.load(file)\n",
    "    \n",
    "    \n",
    "def relations (data_JSON):\n",
    "    relation_type=[]\n",
    "    for document in data_JSON :\n",
    "        entities_with_all=document[\"entities\"]\n",
    "        # relation(r,r1,e1,e2)\n",
    "        relations_with_all=document[\"relations\"]\n",
    "        for idx ,relation_brut in enumerate(relations_with_all):\n",
    "            temp= decapitalize(relation_brut[\"type\"]).replace('-','')+\",\"+decapitalize(entities_with_all[relation_brut[\"head\"]][\"type\"])+\",\"+decapitalize(entities_with_all[relation_brut[\"tail\"]][\"type\"])\n",
    "            if temp not in relation_type:\n",
    "                relation_type.append(\",not group(IdR,\"+temp+\")\")\n",
    "                #print(\"temp\",temp)\n",
    "    return relation_type\n",
    "        \n",
    "    \n",
    "    \n",
    "rel1 =relations (data_JSON1)\n",
    "rel2 = relations (data_JSON2)\n",
    "relation =list(set(rel1) | set(rel2))  \n",
    "relation= ''.join(relation)\n",
    "#print(relation)\n",
    "\n",
    "def retreive_relation(relation) :\n",
    "    relations= [ \"Used-for\", \"Feature-of\", \"Hyponym-of\", \"Evaluate-for\", \"Part-of\" , \"Compare\", \"Conjunction\",\"no\"]\n",
    "    for real in relations:\n",
    "        if relation.lower()==real.replace('-','').lower():\n",
    "            return real\n",
    "\"\"\"\n",
    "\n",
    "#i add relation(R,IdR,IdE1,IdE2), entity(e,IdE1,_,E1), entity(e,IdE2,_,E2), not group(IdR,no,E1,E2) from the previous test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
